{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda env:deepnl]",
      "language": "python",
      "name": "conda-env-deepnl-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "colab": {
      "name": "CS2731-Document_Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kch34/nlp2021_hw1/blob/main/CS2731_Document_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKPl0_xjNU6m"
      },
      "source": [
        "## Document Classification\n",
        "\n",
        "This notebook is designed to get you started thinking about how to use notebooks in a more systematic way as you design machine learning programs for text.  \n",
        "\n",
        "Along the way, we'll see some cool bits of python that you might not have thought of (or known to google for) that make many of the things we'll want to do very easy.  And it includes [some design patterns][1] for exploring representation changes in classification problems that you can work with as you push your ideas forward.\n",
        "\n",
        "[1]:https://en.wikipedia.org/wiki/Software_design_pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWBMQQhANU6o"
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import itertools\n",
        "import vocabulary\n",
        "import newsreader\n",
        "import numpy as np\n",
        "import scipy\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "try:\n",
        "    import cPickle as pickle\n",
        "except:\n",
        "    import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CIHEOraNU6p"
      },
      "source": [
        "## 0. An introduction to using notebooks for research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQn0tBMoNU6p"
      },
      "source": [
        "### Design Pattern 0.0: Keep the things you have to customize separate\n",
        "\n",
        "It's always easy to get carried away getting settled into a particular setup and working on a particular experiment.  But the first step in making your work general is [DRY: don't repeat yourself][1].  That means setting up variables once and for all for the things you're going to have to get right to work with particular data in a particular environment.  Let's get organized.\n",
        "\n",
        "[1]:https://en.wikipedia.org/wiki/Don't_repeat_yourself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_a0ydIqBNU6q"
      },
      "source": [
        "vocab_file, vocab_file_type = \"reviews-vocab.pkl\", \"pickle\"\n",
        "\n",
        "embedding_file, embedding_dimensions, embedding_cache = \\\n",
        "    \"glove.6B.50d.txt\", 50, \"reviews-embedding.npz\"\n",
        "\n",
        "all_data, train_dir, dev_dir, test_dir = \\\n",
        "    \"reviews\", \"reviews/train/\", \"reviews/dev/\", \"reviews/test/\"\n",
        "    \n",
        "class1, class2 = \"pos\", \"neg\"\n",
        "\n",
        "has_bad_metadata = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PrnScObNU6q"
      },
      "source": [
        "### Design Pattern 0.1: Save your work\n",
        "\n",
        "You won't always be able to keep your notebook running indefinitely.  (For one thing, if you are updating basic aspects of the software you're using, the convenions of the python import method means that you'll have to restart the python kernel in order to load the updates.)  On the other hand, some of the computations you want to exploit in dealing with text data are expensive.  So consider using the [python pickle serialization framework][1] and [loading and saving functionality from numpy][2] in cases like this to let you pick up right where you left off.\n",
        "\n",
        "Any time you change the overall parameters of the learning algorithm, set `made_vocabulary` and `made_embedding` to `False`, to reindex the words and get a general word similarity matrix for them.  Once you've done this, set `made_vocabulary` and `made_embedding` to `True`, so you simply load the precomputed representations from the file system. \n",
        "\n",
        "[1]:http://www.ibm.com/developerworks/library/l-pypers/index.html\n",
        "[2]:https://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7PA45UBNU6q"
      },
      "source": [
        "made_vocabulary = True\n",
        "if made_vocabulary :\n",
        "    v = vocabulary.Vocabulary.load(vocab_file, file_type=vocab_file_type)\n",
        "else: \n",
        "    tokens = newsreader.all_textfile_tokens(all_data, strip_metadata=has_bad_metadata)                                            \n",
        "    v = vocabulary.Vocabulary.from_iterable(tokens, file_type=vocab_file_type)\n",
        "    v.save(vocab_file)\n",
        "v.stop_growth()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeOpye5JNU6r"
      },
      "source": [
        "made_embedding = True\n",
        "if made_embedding :\n",
        "    e = newsreader.load_sparse_csr(embedding_cache)\n",
        "else: \n",
        "    e = newsreader.build_sparse_embedding(v, embedding_file, embedding_dimensions)\n",
        "    newsreader.save_sparse_csr(embedding_cache, e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qzpFiDFNU6r"
      },
      "source": [
        "### Design Pattern 0.2: Design your notebook to evolve\n",
        "\n",
        "A notebook is fundamentally a record that you are using to document your work and maintain the rationale for particular programming decisions.  That means you're going to be adapting it.  You want it to be easy to make changes, and you also want it to be possible to read an updated version and understand what's going on.  At the same time, you also want to be able to focus your work on the particular experiments you're exploring at a particular stage in your work, without losing the effort that you've done earlier.\n",
        "\n",
        "Here are some suggestions and infrastructure to make this easier.\n",
        "- Names.  In practice, it's too much effort to give every experiment you do a descriptive name.  Ultimately you have to look at the specific parameters and setup of the experiment anyway, to draw conclusions.  So you're going to wind up using arbitrary elements to distinguish your experiments.  Use names that don't assume a particular order, so you can add new names anywhere, and use names that indicate the dependencies on earlier computations, so you can select specific subparts of the notebook to run.\n",
        "- Sections.  Divide your experiments into sections and use this organization to explain the thinking that you're doing and to find results that you want to go back to.  Consider including the section identifiers strategically in the writeup to help contextualize the documents (for example, the way I'm doing with the design patterns here.)\n",
        "- Limit computations.  The code below allows you to restrict computation to expressions that contribute to the definitions of particular target variables, so you can rerun the entire notebook but just focus on the experiments that you are currently working on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wpwu3o98NU6s"
      },
      "source": [
        "targets = []\n",
        "def selected(name) :\n",
        "    if not targets:\n",
        "        return True\n",
        "    if any(t.startswith(name) for t in targets) :\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFrHS9X5NU6s"
      },
      "source": [
        "### Example 0.0: Finding features\n",
        "\n",
        "You're going to want to use the data manager class, which has placeholders that say how you might build a feature dictionary by using the training data and then how you will scan files to create feature matrices from file tokens.  Here are default functions that carry out those operations in the simplest way possible.  **Remember: simple is good.** Being fancy is just as likely to get you into trouble as it is to help you.  If you have these functions defined, you can make a data manager with a call like this (where `v` is your vocabulary)\n",
        "\n",
        "```python\n",
        "my_data = newsreader.DataManager(class1_training_directory,\n",
        "                                 class2_training_directory,\n",
        "                                 class1_test_directory,\n",
        "                                 class2_test_directory,\n",
        "                                 use_default_features(v), \n",
        "                                 count_features)\n",
        "```\n",
        "\n",
        "Note that this template is set up for 20 newsgroups classification problems, where you're not given an explicit development set to work with, and you'll want to strip the metadata from the files before processing them.  We do something slightly different below for the reviews dataset, since development data is given and there's no metadata to strip from the files. \n",
        "                                 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuqXHT8iNU6t"
      },
      "source": [
        "def use_default_features(vocab) :\n",
        "    return lambda data: vocab\n",
        "\n",
        "def count_features(features, gen_tokens) :\n",
        "    for t in gen_tokens :\n",
        "        r = features.add(t)\n",
        "        if r :\n",
        "            yield r    \n",
        "            \n",
        "count_data = newsreader.DataManager(train_dir + class1,\n",
        "                                       train_dir + class2,\n",
        "                                       test_dir + class1,\n",
        "                                       test_dir + class2,\n",
        "                                       use_default_features(v),\n",
        "                                       count_features,\n",
        "                                       dev_dir + class1 if dev_dir else None,\n",
        "                                       dev_dir + class2 if dev_dir else None,\n",
        "                                       strip_metadata=has_bad_metadata)\n",
        "\n",
        "count_data.initialize(build_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsYC1nFnNU6t"
      },
      "source": [
        "## 1. Representing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jI4u_4QNU6t"
      },
      "source": [
        "### Design Pattern 1.0: Higher-order functions for one-off abstraction\n",
        "\n",
        "We're going to be using `scipy` sparse matrices to handle language data (as we must), but unfortunately it turns out that a lot of the nice operations that you can do on `numpy` matrices don't work on sparse matrices, because they wouldn't preserve the sparsity in the general case.  One of the things that turns out to be impossible is to \"forget\" the  values in a matrix and replace all the nonzero values in a matrix with a sensible value like 1.  Bottom line: if we want to have yes/no features in our data matrices, rather than count features, we're going to have to remove redundancies right away when we scan the file.\n",
        "\n",
        "Now, if you think for a second, you'll realize that this effect doesn't have anything to do with what features we're looking for or how we're looking for them.  Features are always integers, and the way we merge redundancies is to accumulate the features that we've seen so far into a set and, once we're done scanning a file, we then emit the features we observed.\n",
        "\n",
        "Lots of programming languages (but maybe not the ones that you're used to) let you just parameterize this operation by the basic `feature_counter` operation, and create abstract, general code without the nuisance of making objects, classes, data types or other heavy-duty discipline.  Here's how you do it in python.  As it happens, I wanted to experiment with boolean features in exploring the design space for text classifiers today, so this is what I will use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vSiAp9tNU6t"
      },
      "source": [
        "def make_boolean_features(feature_counter) :\n",
        "    def collect_features(features, gen_tokens) :\n",
        "        seen = set()\n",
        "        for f in feature_counter(features, gen_tokens) :\n",
        "            seen.add(f)\n",
        "        for f in seen :\n",
        "            yield f\n",
        "    return collect_features\n",
        "\n",
        "boolean_data = newsreader.DataManager(train_dir + class1,\n",
        "                                         train_dir + class2,\n",
        "                                         test_dir + class1,\n",
        "                                         test_dir + class2,\n",
        "                                         use_default_features(v),\n",
        "                                         make_boolean_features(count_features),\n",
        "                                         dev_dir + class1 if dev_dir else None,\n",
        "                                         dev_dir + class2 if dev_dir else None,\n",
        "                                         strip_metadata=has_bad_metadata)\n",
        "\n",
        "boolean_data.initialize(build_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho3lf0UNNU6u"
      },
      "source": [
        "### Design Pattern 1.1: Bundle your data and provide consistent views\n",
        "\n",
        "From data management, we've learned that analysts are often interested in [views][1], which are systematically related to underlying data but record the results of inferences and other computations which are precomputed so that you can have fast access to them.\n",
        "\n",
        "This is going to be important as you explore the best ways  to represent the underlying text, and design experiments that are going to compare these representations to one another.  \n",
        "\n",
        "This is the kind of thing that it's nice to formalize with a python class.  The `Experiment` class below sets up the problem of learning a classifier from a data manager, arranges the data acquisition and classifier training, and carries out a preliminary validation.  Most importantly, it provides a general method for taking your data, rerepresenting it in a consistent way, and setting up another experiment as a result.\n",
        "\n",
        "### Design Pattern 1.2: But keep the test data sacred\n",
        "\n",
        "**You don't want to make it easy to peek at the test data, which is special and limited.** So even though this class keeps the structure in place so that you manage a consistent representation of the test data, you have to add code to evaluate on the test data yourself.\n",
        "\n",
        "\n",
        "[1]:https://en.wikipedia.org/wiki/View_(SQL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-DmupseNU6u"
      },
      "source": [
        "class Experiment(object) :\n",
        "    '''Organize the process of getting data, building a classifier,\n",
        "    and exploring new representations'''\n",
        "    \n",
        "    def __init__(self, data, comment, classifier, cdesc) :\n",
        "        'set up the problem of learning a classifier from a data manager'\n",
        "        self.data = data\n",
        "        self.comment = comment\n",
        "        self.classifier = classifier\n",
        "        self.cdesc = cdesc\n",
        "        self.initialized = False\n",
        "        \n",
        "    def initialize(self) :\n",
        "        'materialize the training data, dev data and test data as matrices'\n",
        "        if not self.initialized :\n",
        "            self.train_X, self.train_y = self.data.training_data()\n",
        "            self.dev_X, self.dev_y = self.data.dev_data()\n",
        "            self.test_X, self.test_y = self.data.test_data()\n",
        "            self.initialized = True\n",
        "        \n",
        "    def fit_and_validate(self, report=True) :\n",
        "        'train the classifier and assess predictions on dev data'\n",
        "        if not self.initialized :\n",
        "            self.initialize()\n",
        "        self.classifier.fit(self.train_X, self.train_y)\n",
        "        self.dev_predictions = self.classifier.predict(self.dev_X)\n",
        "        self.accuracy = sklearn.metrics.accuracy_score(self.dev_y, self.dev_predictions)\n",
        "        if report :\n",
        "            print(\"{}\\nclassified by {}\\naccuracy {}\".format(self.comment, self.cdesc, self.accuracy))\n",
        "            \n",
        "    def xval(self, folds=20, report=True) :\n",
        "        accuracies = []\n",
        "        for i in range(folds) :\n",
        "            self.fit_and_validate(report=False)\n",
        "            accuracies.append(self.accuracy)\n",
        "        if report :\n",
        "            msg = \"{}\\nclassified by {}\\naverage accuracy {} (std {})\"\n",
        "            print(msg.format(self.comment, self.cdesc, \n",
        "                             sum(accuracies)/folds,\n",
        "                             np.std(accuracies)))\n",
        "    \n",
        "    @classmethod\n",
        "    def transform(cls, expt, operation, description, classifier, cdesc) :\n",
        "        'use operation to transform the data from expt and set up new classifier'\n",
        "        if not expt.initialized :\n",
        "            expt.initialize()\n",
        "        result = cls(expt.data, expt.comment + '\\n' + description, classifier, cdesc)\n",
        "        result.train_X, result.train_y = operation(expt.train_X, expt.train_y, 'train')\n",
        "        result.dev_X, result.dev_y = operation(expt.dev_X, expt.dev_y, 'dev')\n",
        "        result.test_X, result.test_y = operation(expt.test_X, expt.test_y, 'test')\n",
        "        result.initialized = True\n",
        "        return result\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crrxuPrPNU6v"
      },
      "source": [
        "### Example 1.0: Setting up an experiment\n",
        "\n",
        "Pretty straightforward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjxMP2pTNU6v"
      },
      "source": [
        "if selected(\"expt_10_\"):\n",
        "    expt_10_ = Experiment(count_data,\n",
        "                       \"{}: {} vs {}, using word count features\".format(all_data, class1, class2),\n",
        "                       sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                       \"logistic regression\")\n",
        "    expt_10_.initialize()\n",
        "    expt_10_.xval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMSQwmwANU6w"
      },
      "source": [
        "if selected(\"expt_11_\") :\n",
        "    expt_11_ = Experiment(boolean_data,\n",
        "                         \"{}: {} vs {}, using word presence/absence features\".format(all_data, class1, class2),\n",
        "                         sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                         \"logistic regression\")\n",
        "    expt_11_.initialize()\n",
        "    expt_11_.xval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBsZbSCqNU6w"
      },
      "source": [
        "## 2. A Guide to Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1vv9IruNU6w"
      },
      "source": [
        "### Example 2.0: Making feature values meaningful.\n",
        "\n",
        "In preparing this assignment, I read [an ACL 2012 paper by Sida Wang and Chris Manning][2] about text classification.  They suggested weighting word features not based on counts, or 1-0, or based on TF-IDF, but instead based on the evidence that the features provide for the eventual class.  This is a cool and influential idea.  All you need to implement it is to work with binary features, compute a score for each feature, and then reweight by the scores.  It's crazy simple code.\n",
        "* Use boolean indexing and slices to get the data\n",
        "* Sparse matrices have built-in methods for counting nonzero elements\n",
        "* Numpy lets you compute the relevant operations elementwise over the matrix.\n",
        "\n",
        "[2]:http://aclweb.org/anthology/P/P12/P12-2018.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "45fjdABLNU6w"
      },
      "source": [
        "def wang_manning_weights(expt) :\n",
        "    Xyes = expt.train_X[expt.train_y ==1, :]\n",
        "    Xno = expt.train_X[expt.train_y != 1, :] \n",
        "    yesrates = np.log((Xyes.getnnz(axis=0) + 1.) / Xyes.shape[1])\n",
        "    norates = np.log((Xno.getnnz(axis=0) + 1.) / Xno.shape[1])\n",
        "    W = scipy.sparse.diags(yesrates - norates, 0)\n",
        "    return lambda X, y, c: (X.dot(W), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyz-AjXwNU6x"
      },
      "source": [
        "The infrastructure that we have in place already to work with data makes this super-easy to try out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN58pzEZNU6x"
      },
      "source": [
        "if selected(\"expt_11_20_\") :\n",
        "    expt_11_20_ = Experiment.transform(expt_11_,\n",
        "                             wang_manning_weights(expt_11_),\n",
        "                             \"features weighted by evidence they give of class\",\n",
        "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                             \"logistic regression\")\n",
        "    expt_11_20_.xval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUL4zRPkNU6x"
      },
      "source": [
        "This is not just a one-off pattern for this particular research result.  For example, if you don't bother to separate the positive and negative examples, and add a call to [np.reciprocal][1], you'll basically turn this into TFIDF weighting.\n",
        "\n",
        "[1]:https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.reciprocal.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iuh3cl4RNU6x"
      },
      "source": [
        "def idf_weights(expt) :\n",
        "    idf = np.log((expt.train_X.shape[1] + 1.) / (expt.train_X.getnnz(axis=0) + 1.))\n",
        "    W = scipy.sparse.diags(idf, 0)\n",
        "    return lambda X, y, c: (X.dot(W), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dICiXgKpNU6x"
      },
      "source": [
        "if selected(\"expt_10_20_\") :\n",
        "    expt_10_20_ = Experiment.transform(expt_10_,\n",
        "                             idf_weights(expt_10_),\n",
        "                             \"features weighted by inverse document frequency\",\n",
        "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                             \"logistic regression\")\n",
        "    expt_10_20_.xval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49s5U9ojNU6y"
      },
      "source": [
        "### Example 2.1: Drawing on additional resources\n",
        "\n",
        "One of the important ways to improve results on NLP problems is to get access to additional resources.  You always have a limited amount of training data for the problem you're interested in, which will include instances with properties that you have seen rarely if at all.  If you have some general sense of how words behave, from text corpora or dictionaries, you can use that information to generalize from words that you have seen to words that you haven't.\n",
        "\n",
        "Word embeddings, of course, are a prototypical example of this.  To use them, you just add features to your data points computed from the word embeddings.  The sparse matrix `stack` operations make it easy to construct the matrices you need. (Note: if you have extra features that aren't in the word embedding, e.g., bigrams, then you need to account for that in applying the generic word embeddings; conversely, you probably want to add the word embedding features to your problem, since you may actually have information about particular words that gives special but reliable information about your classification problem that you do not want to discard.)  With these hints, you can probably understand the code for adding embedding information to your classification problem.\n",
        "\n",
        "Another thing you could try along these lines would be to build an embedding matrix by applying SVD to the training data, as in the latent semantic analysis exercise we did two weeks ago.\n",
        "\n",
        "```python\n",
        "_, _, wrt = scipy.sparse.linalg.svds(expt.X_train, k=r)\n",
        "W = np.transpose(wrt))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAzfgrY0NU6y"
      },
      "source": [
        "def add_embeddings(expt, embeddings, scale=True, stack=True) :\n",
        "    extra_features = expt.train_X.shape[1] - embeddings.shape[0]\n",
        "    if extra_features > 0 :\n",
        "        Z = scipy.sparse.csr_matrix((extra_features, embeddings.shape[1]))\n",
        "        W = scipy.sparse.vstack([embeddings, Z])\n",
        "    else: \n",
        "        W = embeddings\n",
        "    if scale :\n",
        "        scaler = StandardScaler(with_mean=False)\n",
        "        scaler.fit(expt.train_X.dot(W))\n",
        "    def operation(X, y, s) :\n",
        "        if scale:\n",
        "            new_features = scaler.transform(X.dot(W))\n",
        "        else :\n",
        "            new_features = X.dot(W)\n",
        "        if stack :\n",
        "            all_features = scipy.sparse.hstack([X, new_features]).tocsr()\n",
        "        else :\n",
        "            all_features = new_features\n",
        "        return (all_features, y)\n",
        "    return operation\n",
        "\n",
        "def dimensionality_reduction(expt, dimensions) :\n",
        "    _, _, wrt = scipy.sparse.linalg.svds(expt.train_X, k=dimensions, \n",
        "                                         return_singular_vectors='vh')\n",
        "    return add_embeddings(expt, np.transpose(wrt), stack=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgPWTCZ8NU6y"
      },
      "source": [
        "Here are a few examples: using word embeddings and doing LSA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FeaROd3NU6y"
      },
      "source": [
        "if selected(\"expt_10_211_\") :\n",
        "    expt_10_211_ = Experiment.transform(expt_10_,\n",
        "                             add_embeddings(expt_10_, e),\n",
        "                             \"enriched via word embeddings\",\n",
        "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                            \"logistic regression\")\n",
        "    expt_10_211_.xval()\n",
        "\n",
        "if selected (\"expt_10_212_\") :\n",
        "    expt_10_212_ = Experiment.transform(expt_10_,\n",
        "                             dimensionality_reduction(expt_10_, 100),\n",
        "                             \"transformed via LSA(100)\",\n",
        "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                           \"logistic regression\")\n",
        "    expt_10_212_.xval()\n",
        "    \n",
        "if selected(\"expt_11_211_\") :\n",
        "    expt_11_211_ = Experiment.transform(expt_11_,\n",
        "                             add_embeddings(expt_11_, e),\n",
        "                             \"enriched via word embeddings\",\n",
        "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                            \"logistic regression\")\n",
        "    expt_11_211_.xval()\n",
        " \n",
        "if selected(\"expt_11_20_211_\") :\n",
        "    expt_11_20_211_ = Experiment.transform(expt_11_20_,\n",
        "                             add_embeddings(expt_11_20_, e),\n",
        "                             \"enriched via word embeddings\",\n",
        "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                            \"logistic regression\")\n",
        "    expt_11_20_211_.xval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "167zDWfLNU6y"
      },
      "source": [
        "### Example 2.2: Fancy feature definitions\n",
        "\n",
        "The place that you get a payoff from a good design is when you're able to easily try something out that you didn't anticipate when you started writing the code.  Invariably, that's a bit lucky when it works out, and anything that you start pushing too hard in a direction that it wasn't designed for becomes more and more cumbersome until you can't stand it.  But we're not there yet in this assignment.\n",
        "\n",
        "Last time we looked at [nltk's resources for analyzing collocations][1], because, as we saw, it's an interesting unsupervised technique for characterizing word senses and dealing with ambiguous words.  The architecture that we have for identifying features in the training data makes it possible to preprocess the training data to look for meaningful bigram features.  (The strategy I've used here is not exactly the one that [Wang and Manning][2] recommend by the way!)  Python generator functions make it straightforward to report instances of these features in the course of scanning through a file.  \n",
        "\n",
        "So it's totally routine to try this out by adapting the work we've done so far.  Who knows what else we could handle this way!\n",
        "\n",
        "[1]:http://www.nltk.org/howto/collocations.html\n",
        "[2]:http://aclweb.org/anthology/P/P12/P12-2018.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKVQW-_HNU6z"
      },
      "source": [
        "def use_bigram_features(data) :\n",
        "    f = vocabulary.Vocabulary.load(vocab_file, file_type=vocab_file_type)\n",
        "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "    word_fd = nltk.FreqDist(data.all_train_tokens())\n",
        "    bigram_fd = nltk.FreqDist(nltk.bigrams(data.all_train_tokens()))\n",
        "    finder = nltk.collocations.BigramCollocationFinder(word_fd, bigram_fd)\n",
        "    finder.apply_freq_filter(5)\n",
        "    collocations = filter(lambda (g,i): i > 0, finder.score_ngrams(bigram_measures.pmi))\n",
        "    for (w1, w2), _ in collocations:\n",
        "        f.add(w1 + \" \" + w2)\n",
        "    f.stop_growth()\n",
        "    return f\n",
        "\n",
        "def count_bigram_features(features, gen_tokens) :\n",
        "    prev = None\n",
        "    for t in gen_tokens :\n",
        "        r = features.add(t)\n",
        "        if r :\n",
        "            yield r\n",
        "            if prev :\n",
        "                r = features.add(prev + \" \" + t)\n",
        "                if r : \n",
        "                    yield r\n",
        "            prev = t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMVeT8QoNU6z"
      },
      "source": [
        "In order to test this, we first need to reload the data with the new feature definitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FskAxhHBNU6z"
      },
      "source": [
        "bigram_data = newsreader.DataManager(train_dir + class1,\n",
        "                                       train_dir + class2,\n",
        "                                       test_dir + class1,\n",
        "                                       test_dir + class2,\n",
        "                                       use_bigram_features,\n",
        "                                       make_boolean_features(count_bigram_features),\n",
        "                                       dev_dir + class1 if dev_dir else None,\n",
        "                                       dev_dir + class2 if dev_dir else None,\n",
        "                                       strip_metadata=has_bad_metadata)\n",
        "\n",
        "bigram_data.initialize(build_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu7rzsEcNU6z"
      },
      "source": [
        "As before, we create a basic experiment to build a classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb_cG5y_NU6z"
      },
      "source": [
        "if selected(\"expt_22_\") :\n",
        "    expt_22_ = Experiment(bigram_data,\n",
        "                   \"{}: {} vs {}, using word and bigram presence/absence features\".format(all_data, class1, class2),\n",
        "                   sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                   \"logistic regression\")\n",
        "    expt_22_.initialize()\n",
        "    expt_22_.xval()\n",
        "\n",
        "if selected(\"expt_22_220_\") :\n",
        "    expt_22_220_ = Experiment.transform(expt_22_,\n",
        "                             wang_manning_weights(expt_22_),\n",
        "                             \"features weighted by evidence they give of class\",\n",
        "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                             \"logistic regression\")\n",
        "    expt_22_220_.initialize()\n",
        "    expt_22_220_.xval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kQP5bM9NU6z"
      },
      "source": [
        "We can compare the basic version to a version with embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyzYmsb8NU6z"
      },
      "source": [
        "if selected(\"expt_22_221_\") :\n",
        "    expt_22_221_ = Experiment.transform(expt_22_,\n",
        "                             add_embeddings(expt_22_, e),\n",
        "                             \"with word embedding features added\",\n",
        "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                            \"logistic regression\")\n",
        "    expt_22_221_.xval()\n",
        "\n",
        "if selected(\"expt_22_220_221_\") :\n",
        "    expt_22_220_221_ = Experiment.transform(expt_22_220_,\n",
        "                             add_embeddings(expt_22_220_, e),\n",
        "                             \"with word embedding features added\",\n",
        "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                            \"logistic regression\")\n",
        "    expt_22_220_221_.xval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oosUIPtnNU60"
      },
      "source": [
        "### Example 2.3 Understanding the learning curve\n",
        "\n",
        "Different feature representations may work better depending on the amount of training data available.  The transformation setup we have makes it easy to consider just a small amount of training data -- by selecting a random subset of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Zo1s-yTLNU60"
      },
      "source": [
        "def limit_training(percent) :\n",
        "    def operation(X, y, s) :\n",
        "        if s != 'train' :\n",
        "            return (X, y)\n",
        "        data_to_take = int(X.shape[0] * percent)\n",
        "        indices = np.random.choice(X.shape[0], \n",
        "                                      size=data_to_take,\n",
        "                                      replace=False)\n",
        "        return (X[indices,:], y[indices])\n",
        "    return operation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-Dzph9NU60"
      },
      "source": [
        "Here's an illustration -- how much do the embeddings help with only 10% of the training data.  One issue is that the fitting is very noisy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tuj2UjSNU60"
      },
      "source": [
        "if selected(\"expt_11_231_\") :\n",
        "    expt_11_231_ = Experiment.transform(expt_11_,\n",
        "                             limit_training(0.1),\n",
        "                             \"considering 10% training data\",\n",
        "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                            \"logistic regression\")\n",
        "    expt_11_231_.xval()\n",
        "\n",
        "if selected(\"expt_11_211_232_\") :\n",
        "    expt_11_231_232_ = Experiment.transform(expt_11_211_,\n",
        "                             limit_training(0.1),\n",
        "                             \"considering 10% training data\",\n",
        "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                            \"logistic regression\")\n",
        "    expt_11_231_232_.xval()\n",
        "\n",
        "\n",
        "if selected(\"expt_11_20_233_\") :\n",
        "    expt_11_20_232_ = Experiment.transform(expt_11_20_,\n",
        "                             limit_training(0.1),\n",
        "                             \"considering 10% training data\",\n",
        "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                            \"logistic regression\")\n",
        "    expt_11_20_232_.xval()\n",
        "\n",
        "if selected(\"expt_11_20_211_234_\") :\n",
        "    expt_11_20_211_232_ = Experiment.transform(expt_11_20_211_,\n",
        "                             limit_training(0.1),\n",
        "                             \"considering 10% training data\",\n",
        "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
        "                                       penalty=\"elasticnet\",\n",
        "                                       n_iter=50),\n",
        "                            \"logistic regression\")\n",
        "    expt_11_20_211_232_.xval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mKepQxNRNU60"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}